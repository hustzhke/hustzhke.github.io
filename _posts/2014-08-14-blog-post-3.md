---
title: 'Blog Post number 3'
date: 2014-08-14
permalink: /posts/2014/08/blog-post-3/
tags:
  - cool posts
  - category1
  - category2
---


# 著名深度学习论文

深度学习论文成千上万，从哪里开始呢？这里是精心挑选的最热门论文列表。

|年份|作者/标题|注释|
|---|---|---|
|1943|Warren S. McCulloch和Walter Pitts，*A Logical Calculus of the Ideas Immanent in Nervous Activity*|神经网络是计算机器吗？McCulloch和Pitts首次将神经网络建模为抽象计算系统。他们发现，在各种假设下，神经元网络与命题逻辑一样强大，引发了人们对神经计算模型的广泛兴趣。|
|1958|Frank Rosenblatt，*The Perceptron: A Probablistic Model for Information Storage and Organization in the Brain*|人工神经网络能学习吗？Rosenblatt提出了感知机算法，这是一种通过迭代调整神经元之间可变权重连接来学习解决问题的方法。他从美国海军获得资金来建造一台物理感知机。在新闻报道中，Rosenblatt预期会出现行走、说话、有自我意识的机器。|
|1959|Jerome Lettvin、Humberto Maturana、Warren McCulloch和Walter Pitts，*What the Frog's Eye Tells the Frog's Brain*|神经传递思想吗？Lettvin挑衅性地提出，青蛙视神经传达的是有意义模式的存在，而不仅仅是亮度，证明眼睛在视觉计算工作中发挥了部分作用。Lettvin还以他著名的思想实验而闻名，即你的大脑可能包含一个“祖母神经元”，你用它来概念化你的祖母。|
|1959|David H. Hubel和Torsten N Wiesel，*Receptive Fields of Single Neurones in the Cat's Striate Cortex*|生物视觉是如何工作的？这篇论文及其1962年的扩展开启了Hubel和Wiesel为期25年的合作，他们系统地分析了哺乳动物视觉系统中的信号处理，对视觉皮层的运作产生了许多具体见解，后来启发并影响了卷积神经网络的设计。他们于1981年获得诺贝尔奖。|
|1969|Marvin Minsky和Seymour Papert，*Perceptrons: An Introduction to Computational Geometry*|感知机不能学习什么？在20世纪60年代早期，当Rosenblatt认为他的神经网络几乎可以做任何事情时，Minsky反驳说它们能做的很少。这本有影响力的书阐述了负面论点，表明许多简单问题，如迷宫求解甚至异或问题，都不能由单层感知机网络解决。这一尖锐批评导致了第一次人工智能寒冬，许多研究人员放弃了神经网络。|
|1972|Teuvo Kohonen，*Correlation Matrix Memories*|神经网络能存储记忆吗？Kohonen（与Anderson同时）观察到，如果将键和数据视为神经激活向量，并且键是线性独立的，单层网络可以充当矩阵联想记忆。联想记忆将成为未来几十年神经网络研究的主要焦点。|
|1981|Geoffrey E. Hinton，*Implementing Semantic Networks in Parallel Hardware*|概念如何表示？在与Anderson合著的一本关于联想记忆的书中，Hinton提出概念不应表示为单个单元，而应表示为激活向量，他展示了一种以分布式方式编码复杂关系的方案。分布式表示成为并行分布式处理（PDP）框架的核心原则，在Rumelhart、McCleland和Hinton随后于1986年出版的一本书中得到发展，并成为理解大型神经网络的核心教条。|
|1986|David E. Rumelhart、Geoffrey E. Hinton和Ronald J. Williams，*Learning Representations by Back-Propagating Errors*|深度网络如何学习？在这篇论文解释反向传播方法之前，多层网络中的学习尚未被广泛理解，该方法通过有效计算梯度来更新权重。虽然Griewank（2012）指出反向模式自动微分已被多次独立发现，特别是由Seppo Linnainmaa（1970）和Paul Werbos（1981）发现，但Rumelhart给《自然》杂志的信展示了其学习非平凡表示的能力，引起了广泛关注，并引发了神经网络的新一轮创新。|
|1988|Sarah Solla、Esther Levin和Michael Fleisher，*Accelerated Learning in Layered Neural Networks*|深度网络应该学习什么？在三篇同时发表的论文中，Solla等人、John Hopfield（1987）和Eric Baum与Frank Wilczek（1988）描述了一种观点，即神经网络通常应该计算对数概率，而不仅仅是任意数量级的数字，并且交叉熵目标通常比平方误差最小化更自然、更有效。（其有效性仍然是一个开放的研究领域：见Hui 2021和Golik 2013。）|
|1989|Yann Le Cun、B. Boser、J. S. Denker、D. Henderson、R. E. Howard、W. Hubbard和L. D. Jackel，*Handwritten Digit Recognition with a Back-Propagation Network*|深度网络能学会看吗？在一项技术壮举中，Le Cun设计了卷积神经网络（CNN）（受Hubel和Weisel生物学研究的启发和影响），并证明反向传播可以训练CNN准确读取美国邮政邮件地址上的手写数字。这项工作展示了良好网络架构的价值，并证明深度网络可以解决现实世界的问题。也可参阅Fukushima 1980了解该想法的早期变体。|
|1990|Jeffrey L Elman，*Finding Structure in Time*|深度网络能学习语言吗？Elman采用了Michael Jordan（1986）设计的三层递归神经网络（RNN）架构，训练RNN从字母开始对自然语言文本进行建模。令人惊讶的是，他发现网络学会了表示单词的结构、语法和语义元素。|
|1990|Léon Bottou和Patrick Gallinari，*A Framework for the Cooperation of Learning Algorithms*|神经网络架构的正确表示法是什么？Bottou观察到反向传播算法允许一种优雅的图形表示法，其中网络不是写成神经元图，而是写成封装向量化前向和反向梯度计算的计算模块图。Bottou的模块化思想是深度学习库如Torch（Collobert 2002）、Theano（Bergstra 2010）、Caffe（Jia 2014）、Tensorflow（Abadi 2016）、JAX（Frostig 2018）和PyTorch（Paszke 2019）的基础。|
|1989|George Cybenko，*Approximation by Superpositions of a Sigmoidal Function*|深度网络能计算什么函数？这篇论文证明，在有限域上，任何连续函数都可以用神经网络以任意小的误差进行逼近。Cybenko的推理特定于当时流行的Sigmoid非线性，但Hornik（1991）表明该结果可以推广到基本上任何普通非线性，并且两层就足够了。Cybenko和Hornik的结果表明，多层网络是通用逼近器，比50年代和60年代提出的单层感知机更具表现力。|
|1991|Léon Bottou，*Stochastic Gradient Learning in Neural Networks*|应该使用什么优化算法？在他的博士论文中，Bottou提出先前提出的学习算法如感知机对应于随机梯度下降（SGD），并认为SGD比更复杂的高阶优化方法更具扩展性。几十年来，Bottou被证明是正确的，简单SGD算法的变体成为神经网络的标准主力学习算法。有关Bottou对SGD的最新讨论，请参阅Bottou（1998）和Bottou（2010），也可参阅Zinkevich（2003）了解一个优雅的可推广收敛证明。|
|1991|Anders Krogh和John A. Hertz，*A Simple Weight Decay Can Improve Generalization*|如何避免过拟合？这篇论文分析并倡导权重衰减，这是一种最初作为岭回归（Hoerl，1970）提出的简单正则化方法，对模型权重的平方施加惩罚。Krogh在神经网络中分析了这个技巧，证明了在单层和多层网络中泛化能力的提升。|
|1997|Sepp Hochreiter和Jürgen Schmidhuber，*Long Short-Term Memory*|如何稳定长序列的递归？如果不采取特殊措施，多次迭代RNN必然会导致梯度爆炸。这篇论文提出了长短期记忆（LSTM）架构，这是一种门控但可微的神经记忆结构，可以在很长的序列中保留状态，同时防止梯度爆炸。LSTM架构还启发了门控循环单元（GRU），这是Cho在2014年设计的一种更简单的替代方案。|
|2003|Yoshua Bengio、Réjean Ducharme、Pascal Vincent和Christian Jauvin，*A Neural Probabilistic Language Model*|神经网络能否大规模建模语言？在这项工作中，Bengio的团队将非递归神经语言模型扩展到1500万字的训练集，大幅超越了当时最先进的传统语言建模方法。Bengio没有使用完全递归网络，而是处理固定的n个单词窗口，并使用一个网络层来学习位置无关的词嵌入。|
|2005|Rodrigo Quian Quiroga、Leila Reddy、Gabriel Kreiman、Christof Koch和Itzhak Fried，*Invariant Visual Representation by Single Neurons in the Human Brain*|单个生物神经元的作用是什么？在一系列对人类癫痫患者单个神经元的出色实验中，发现了几个多模态神经元：对唤起相同概念的非常不同刺激有选择性反应的单个神经元，例如，一个神经元对Halle Berry的书面名字、草图、照片或穿着戏服的形象有反应，而对其他人没有反应，这表明大脑中高级概念的简单物理编码。|
|2005|Geoffrey Hinton，*What Kind of Graphical Model is the Brain?*|网络能否像自旋玻璃一样加深？在21世纪初，神经网络研究专注于将网络扩展到三层以上的问题。一个突破来自受自旋玻璃物理学启发的神经网络双向链接模型，如Hopfield网络（Hopfield，1982）和受限玻尔兹曼机（RBM）（Hinton，1983）。2005年，Hinton表明一种称为深度信念网络的RBM可以有效地训练多层堆叠，2006年，Hinton和Salakhutdinov表明，如果用RBM初始化，自动编码器层可以堆叠。|
|2010|Pascal Vincent、Hugo Larochelle、Isabelle Lajoie、Yoshua Bengio和Pierre-Antoine Manzagol，*Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion*|能否通过无监督训练加深网络？对更简单的深度网络初始化方法的探索仍在继续，2010年，Vincent找到了一种替代玻尔兹曼机初始化的方法：将每一层训练为去噪自动编码器，必须学习去除添加到训练数据中的噪声。该小组还设计了收缩自动编码器（Rifai，2011），其中在损失中加入了梯度惩罚。|
|2010|Xavier Glorot和Yoshua Bengio，*Understanding the Difficulty of Training Deep Feedforward Neural Networks*|能否通过简单改变加深网络？Glorot分析了普通前馈训练的问题，并提出了Xavier初始化，这是一种简单的随机初始化，缩放以避免梯度消失或爆炸。在第二个重要发展中，Nair（2010）和Glorot（2011）通过实验发现整流线性单元（ReLU）比以前普遍使用的Sigmoid非线性函数效果好得多。这些易于应用的创新消除了对复杂预训练的需求，因此深度前馈网络可以使用反向传播直接从头开始端到端训练。|
|2011|Ronan Collobert、Jason Weston、Léon Bottou、Michael Karlen、Koray Kavukcuoglu和Pavel Kuksa，*Natural Language Processing (Almost) from Scratch*|神经网络能否解决语言问题？以前的自然语言处理工作将分块、词性标注、命名实体识别和语义角色标注等问题分开处理。Collobert声称单个神经网络可以一次性完成所有这些任务，使用多任务目标为所有任务学习统一的语言表示。他们发现他们的网络学习到了令人满意的词嵌入，将有意义相关的单词分组在一起，但最初的性能声称受到怀疑。|
|2012|Alex Krizhevsky、Ilya Sutskever和Geoffrey E. Hinton，*ImageNet Classification with Deep Convolutional Neural Networks*|神经网络能否实现最先进的计算机视觉？Krizhevsy用一个深度卷积网络震惊了计算机视觉界，该网络在年度ImageNet分类挑战（Deng，2009）中大幅获胜。Krizhevsky的AlexNet是一个深度为八层、有6000万个参数的卷积网络，结合了ReLU和Dropout（Srivatsava，2014和Hinton，2012）等最新技巧，并在一对消费级图形处理单元（GPU）上运行。在备受瞩目的大规模基准上的卓越性能引发了机器学习社区对神经网络观点的突然转变，以及对深度网络应用的兴趣爆发式复苏。|
|2012|Tomas Mikolov、Illya Sutskever、Kai Chen、Greg Corrado和Jeffrey Dean，*Distributed Representations of Words and Phrases and their Compositionality*|大量数据能否胜过复杂网络？随着对神经网络力量的兴奋情绪增长，谷歌研究员Mikolov发现他的简单（非深度）跳字模型（Mikolov，2012a）如果在300亿字的大规模数据集上进行训练，可以学习到比其他（深度）嵌入好得多的词嵌入。这个Word2Vec模型首次展示了语义向量组合。谷歌还使用拓扑独立成分分析损失（Hyvärinen 2009）在Youtube图像数据上训练了一个无监督模型（Le，2011），并观察到了人脸和猫的单个神经元的出现。|
|2013|Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Alex Graves、Ioannis Antonoglou、Daan Wierstra和Martin Riedmiller，*Playing Atari with Deep Reinforcement Learning*|网络能否从原始输入学习玩游戏？DeepMind提出深度强化学习（DRL），将神经网络直接应用于Q学习算法，并证明他们的深度Q网络（DQN）架构可以直接从状态观察预测动作，并且可以学习控制操纵杆，足以学会比人类更好地玩几款Atari游戏。这项工作激发了许多其他DRL方法，如深度确定性策略梯度（DDPG）（Lillicrap 2016）和近端策略优化（PPO）（Shulman 2017），并引发了Atari能力的RL测试环境如OpenAI Gym的开发。|
|2013|Diederik P. Kingma和Max Welling，*Auto-Encoding Variational Bayes*|自动编码器应该重建什么？变分自编码器（VAE）将自动编码器视为变分推断问题，通过最小化随机潜在变量中的信息来最大化数据似然的证据下界（ELBO），从而匹配分布而不是实例，并使用重参数化技巧在瓶颈处训练采样过程（见Doersch教程）。VAE的灵感来自Hinton 1995年的唤醒 - 睡眠算法，该算法解决了学习连续潜在变量模型的相同问题。其后代如Beta-VAE（Higgins 2017）可以学习解缠结表示，VQ-VAE（van der Oord 2017）可以进行最先进的图像生成。|
|2013|Christian Szegedy、Wojciech Zaremba、Ilya Sutskever、Joan Bruna、Dumitru Erhan、Ian Goodfellow和Rob Fergus，*Intriguing Properties of Neural Networks*|人工神经网络有缺陷吗？通过简单优化，Szegedy发现很容易构造对抗样本：与自然输入几乎没有区别但能欺骗深度网络错误分类图像的输入。这一观察引发了进一步攻击（如Papernot 2017）、防御（Madry 2018）和评估（Carlini 2017）的发现。|
|2014|Ross Girshick、Jeff Donahue、Trevor Darrell和Jitendra Malik，*Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation*|CNN能否在场景中定位对象？计算机视觉不仅关注分类，还关注在场景中定位和理解对象的排列。通过利用CNN特征的空间排列，Girshick的R-CNN（以及Ren 2015年的更快R-CNN）不仅可以识别对象的类别，还可以通过边界框估计和语义分割识别对象在场景中的位置。|
|2014|Ian J. Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David Warde-Farley、Sherjil Ozair、Aaron Courville、Yoshua Bengio和Jitendra Malik，*Generative Adversarial Nets*|对抗目标能否被学习？生成对抗网络（GAN）通过学习合成能欺骗同时训练以区分真实和生成数据的第二个对抗模型的示例来模仿数据集。这种优雅的方法引发了一波新的理论工作，以及一类新的高度逼真的图像生成方法，如DCGAN（Radford 2016）、Wasserstein GAN（Arjovsky 2017）、BigGAN（Brock 2019）和StyleGAN（Karras 2019）。|
|2014|Jason Yosinksi、Jeff Clune、Yoshua Bengio和Hod Lipson，*How Transferable are Features in Deep Neural Networks?*|网络参数能否在另一个网络中重用？迁移学习使用预训练网络的层来初始化







This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. 

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------
