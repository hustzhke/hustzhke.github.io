---
title: '著名深度学习论文'
date: 2025-01-03
permalink: /posts/2014/08/blog-post-3/
tags:
  - cool posts
  - category1
  - category2
---




|年份|作者/标题|注释|
|---|---|---|
|1943|Warren S. McCulloch和Walter Pitts，*A Logical Calculus of the Ideas Immanent in Nervous Activity*|神经网络是计算机器吗？McCulloch和Pitts首次将神经网络建模为抽象计算系统。他们发现，在各种假设下，神经元网络与命题逻辑一样强大，引发了人们对神经计算模型的广泛兴趣。|
|1958|Frank Rosenblatt，*The Perceptron: A Probablistic Model for Information Storage and Organization in the Brain*|人工神经网络能学习吗？Rosenblatt提出了感知机算法，这是一种通过迭代调整神经元之间可变权重连接来学习解决问题的方法。他从美国海军获得资金来建造一台物理感知机。在新闻报道中，Rosenblatt预期会出现行走、说话、有自我意识的机器。|
|1959|Jerome Lettvin、Humberto Maturana、Warren McCulloch和Walter Pitts，*What the Frog's Eye Tells the Frog's Brain*|神经传递思想吗？Lettvin挑衅性地提出，青蛙视神经传达的是有意义模式的存在，而不仅仅是亮度，证明眼睛在视觉计算工作中发挥了部分作用。Lettvin还以他著名的思想实验而闻名，即你的大脑可能包含一个“祖母神经元”，你用它来概念化你的祖母。|
|1959|David H. Hubel和Torsten N Wiesel，*Receptive Fields of Single Neurones in the Cat's Striate Cortex*|生物视觉是如何工作的？这篇论文及其1962年的扩展开启了Hubel和Wiesel为期25年的合作，他们系统地分析了哺乳动物视觉系统中的信号处理，对视觉皮层的运作产生了许多具体见解，后来启发并影响了卷积神经网络的设计。他们于1981年获得诺贝尔奖。|
|1969|Marvin Minsky和Seymour Papert，*Perceptrons: An Introduction to Computational Geometry*|感知机不能学习什么？在20世纪60年代早期，当Rosenblatt认为他的神经网络几乎可以做任何事情时，Minsky反驳说它们能做的很少。这本有影响力的书阐述了负面论点，表明许多简单问题，如迷宫求解甚至异或问题，都不能由单层感知机网络解决。这一尖锐批评导致了第一次人工智能寒冬，许多研究人员放弃了神经网络。|
|1972|Teuvo Kohonen，*Correlation Matrix Memories*|神经网络能存储记忆吗？Kohonen（与Anderson同时）观察到，如果将键和数据视为神经激活向量，并且键是线性独立的，单层网络可以充当矩阵联想记忆。联想记忆将成为未来几十年神经网络研究的主要焦点。|
|1981|Geoffrey E. Hinton，*Implementing Semantic Networks in Parallel Hardware*|概念如何表示？在与Anderson合著的一本关于联想记忆的书中，Hinton提出概念不应表示为单个单元，而应表示为激活向量，他展示了一种以分布式方式编码复杂关系的方案。分布式表示成为并行分布式处理（PDP）框架的核心原则，在Rumelhart、McCleland和Hinton随后于1986年出版的一本书中得到发展，并成为理解大型神经网络的核心教条。|
|1986|David E. Rumelhart、Geoffrey E. Hinton和Ronald J. Williams，*Learning Representations by Back-Propagating Errors*|深度网络如何学习？在这篇论文解释反向传播方法之前，多层网络中的学习尚未被广泛理解，该方法通过有效计算梯度来更新权重。虽然Griewank（2012）指出反向模式自动微分已被多次独立发现，特别是由Seppo Linnainmaa（1970）和Paul Werbos（1981）发现，但Rumelhart给《自然》杂志的信展示了其学习非平凡表示的能力，引起了广泛关注，并引发了神经网络的新一轮创新。|
|1988|Sarah Solla、Esther Levin和Michael Fleisher，*Accelerated Learning in Layered Neural Networks*|深度网络应该学习什么？在三篇同时发表的论文中，Solla等人、John Hopfield（1987）和Eric Baum与Frank Wilczek（1988）描述了一种观点，即神经网络通常应该计算对数概率，而不仅仅是任意数量级的数字，并且交叉熵目标通常比平方误差最小化更自然、更有效。（其有效性仍然是一个开放的研究领域：见Hui 2021和Golik 2013。）|
|1989|Yann Le Cun、B. Boser、J. S. Denker、D. Henderson、R. E. Howard、W. Hubbard和L. D. Jackel，*Handwritten Digit Recognition with a Back-Propagation Network*|深度网络能学会看吗？在一项技术壮举中，Le Cun设计了卷积神经网络（CNN）（受Hubel和Weisel生物学研究的启发和影响），并证明反向传播可以训练CNN准确读取美国邮政邮件地址上的手写数字。这项工作展示了良好网络架构的价值，并证明深度网络可以解决现实世界的问题。也可参阅Fukushima 1980了解该想法的早期变体。|
|1990|Jeffrey L Elman，*Finding Structure in Time*|深度网络能学习语言吗？Elman采用了Michael Jordan（1986）设计的三层递归神经网络（RNN）架构，训练RNN从字母开始对自然语言文本进行建模。令人惊讶的是，他发现网络学会了表示单词的结构、语法和语义元素。|
|1990|Léon Bottou和Patrick Gallinari，*A Framework for the Cooperation of Learning Algorithms*|神经网络架构的正确表示法是什么？Bottou观察到反向传播算法允许一种优雅的图形表示法，其中网络不是写成神经元图，而是写成封装向量化前向和反向梯度计算的计算模块图。Bottou的模块化思想是深度学习库如Torch（Collobert 2002）、Theano（Bergstra 2010）、Caffe（Jia 2014）、Tensorflow（Abadi 2016）、JAX（Frostig 2018）和PyTorch（Paszke 2019）的基础。|
|1989|George Cybenko，*Approximation by Superpositions of a Sigmoidal Function*|深度网络能计算什么函数？这篇论文证明，在有限域上，任何连续函数都可以用神经网络以任意小的误差进行逼近。Cybenko的推理特定于当时流行的Sigmoid非线性，但Hornik（1991）表明该结果可以推广到基本上任何普通非线性，并且两层就足够了。Cybenko和Hornik的结果表明，多层网络是通用逼近器，比50年代和60年代提出的单层感知机更具表现力。|
|1991|Léon Bottou，*Stochastic Gradient Learning in Neural Networks*|应该使用什么优化算法？在他的博士论文中，Bottou提出先前提出的学习算法如感知机对应于随机梯度下降（SGD），并认为SGD比更复杂的高阶优化方法更具扩展性。几十年来，Bottou被证明是正确的，简单SGD算法的变体成为神经网络的标准主力学习算法。有关Bottou对SGD的最新讨论，请参阅Bottou（1998）和Bottou（2010），也可参阅Zinkevich（2003）了解一个优雅的可推广收敛证明。|
|1991|Anders Krogh和John A. Hertz，*A Simple Weight Decay Can Improve Generalization*|如何避免过拟合？这篇论文分析并倡导权重衰减，这是一种最初作为岭回归（Hoerl，1970）提出的简单正则化方法，对模型权重的平方施加惩罚。Krogh在神经网络中分析了这个技巧，证明了在单层和多层网络中泛化能力的提升。|
|1997|Sepp Hochreiter和Jürgen Schmidhuber，*Long Short-Term Memory*|如何稳定长序列的递归？如果不采取特殊措施，多次迭代RNN必然会导致梯度爆炸。这篇论文提出了长短期记忆（LSTM）架构，这是一种门控但可微的神经记忆结构，可以在很长的序列中保留状态，同时防止梯度爆炸。LSTM架构还启发了门控循环单元（GRU），这是Cho在2014年设计的一种更简单的替代方案。|
|2003|Yoshua Bengio、Réjean Ducharme、Pascal Vincent和Christian Jauvin，*A Neural Probabilistic Language Model*|神经网络能否大规模建模语言？在这项工作中，Bengio的团队将非递归神经语言模型扩展到1500万字的训练集，大幅超越了当时最先进的传统语言建模方法。Bengio没有使用完全递归网络，而是处理固定的n个单词窗口，并使用一个网络层来学习位置无关的词嵌入。|
|2005|Rodrigo Quian Quiroga、Leila Reddy、Gabriel Kreiman、Christof Koch和Itzhak Fried，*Invariant Visual Representation by Single Neurons in the Human Brain*|单个生物神经元的作用是什么？在一系列对人类癫痫患者单个神经元的出色实验中，发现了几个多模态神经元：对唤起相同概念的非常不同刺激有选择性反应的单个神经元，例如，一个神经元对Halle Berry的书面名字、草图、照片或穿着戏服的形象有反应，而对其他人没有反应，这表明大脑中高级概念的简单物理编码。|
|2005|Geoffrey Hinton，*What Kind of Graphical Model is the Brain?*|网络能否像自旋玻璃一样加深？在21世纪初，神经网络研究专注于将网络扩展到三层以上的问题。一个突破来自受自旋玻璃物理学启发的神经网络双向链接模型，如Hopfield网络（Hopfield，1982）和受限玻尔兹曼机（RBM）（Hinton，1983）。2005年，Hinton表明一种称为深度信念网络的RBM可以有效地训练多层堆叠，2006年，Hinton和Salakhutdinov表明，如果用RBM初始化，自动编码器层可以堆叠。|
|2010|Pascal Vincent、Hugo Larochelle、Isabelle Lajoie、Yoshua Bengio和Pierre-Antoine Manzagol，*Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion*|能否通过无监督训练加深网络？对更简单的深度网络初始化方法的探索仍在继续，2010年，Vincent找到了一种替代玻尔兹曼机初始化的方法：将每一层训练为去噪自动编码器，必须学习去除添加到训练数据中的噪声。该小组还设计了收缩自动编码器（Rifai，2011），其中在损失中加入了梯度惩罚。|
|2010|Xavier Glorot和Yoshua Bengio，*Understanding the Difficulty of Training Deep Feedforward Neural Networks*|能否通过简单改变加深网络？Glorot分析了普通前馈训练的问题，并提出了Xavier初始化，这是一种简单的随机初始化，缩放以避免梯度消失或爆炸。在第二个重要发展中，Nair（2010）和Glorot（2011）通过实验发现整流线性单元（ReLU）比以前普遍使用的Sigmoid非线性函数效果好得多。这些易于应用的创新消除了对复杂预训练的需求，因此深度前馈网络可以使用反向传播直接从头开始端到端训练。|
|2011|Ronan Collobert、Jason Weston、Léon Bottou、Michael Karlen、Koray Kavukcuoglu和Pavel Kuksa，*Natural Language Processing (Almost) from Scratch*|神经网络能否解决语言问题？以前的自然语言处理工作将分块、词性标注、命名实体识别和语义角色标注等问题分开处理。Collobert声称单个神经网络可以一次性完成所有这些任务，使用多任务目标为所有任务学习统一的语言表示。他们发现他们的网络学习到了令人满意的词嵌入，将有意义相关的单词分组在一起，但最初的性能声称受到怀疑。|
|2012|Alex Krizhevsky、Ilya Sutskever和Geoffrey E. Hinton，*ImageNet Classification with Deep Convolutional Neural Networks*|神经网络能否实现最先进的计算机视觉？Krizhevsy用一个深度卷积网络震惊了计算机视觉界，该网络在年度ImageNet分类挑战（Deng，2009）中大幅获胜。Krizhevsky的AlexNet是一个深度为八层、有6000万个参数的卷积网络，结合了ReLU和Dropout（Srivatsava，2014和Hinton，2012）等最新技巧，并在一对消费级图形处理单元（GPU）上运行。在备受瞩目的大规模基准上的卓越性能引发了机器学习社区对神经网络观点的突然转变，以及对深度网络应用的兴趣爆发式复苏。|
|2012|Tomas Mikolov、Illya Sutskever、Kai Chen、Greg Corrado和Jeffrey Dean，*Distributed Representations of Words and Phrases and their Compositionality*|大量数据能否胜过复杂网络？随着对神经网络力量的兴奋情绪增长，谷歌研究员Mikolov发现他的简单（非深度）跳字模型（Mikolov，2012a）如果在300亿字的大规模数据集上进行训练，可以学习到比其他（深度）嵌入好得多的词嵌入。这个Word2Vec模型首次展示了语义向量组合。谷歌还使用拓扑独立成分分析损失（Hyvärinen 2009）在Youtube图像数据上训练了一个无监督模型（Le，2011），并观察到了人脸和猫的单个神经元的出现。|
|2013|Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Alex Graves、Ioannis Antonoglou、Daan Wierstra和Martin Riedmiller，*Playing Atari with Deep Reinforcement Learning*|网络能否从原始输入学习玩游戏？DeepMind提出深度强化学习（DRL），将神经网络直接应用于Q学习算法，并证明他们的深度Q网络（DQN）架构可以直接从状态观察预测动作，并且可以学习控制操纵杆，足以学会比人类更好地玩几款Atari游戏。这项工作激发了许多其他DRL方法，如深度确定性策略梯度（DDPG）（Lillicrap 2016）和近端策略优化（PPO）（Shulman 2017），并引发了Atari能力的RL测试环境如OpenAI Gym的开发。|
|2013|Diederik P. Kingma和Max Welling，*Auto-Encoding Variational Bayes*|自动编码器应该重建什么？变分自编码器（VAE）将自动编码器视为变分推断问题，通过最小化随机潜在变量中的信息来最大化数据似然的证据下界（ELBO），从而匹配分布而不是实例，并使用重参数化技巧在瓶颈处训练采样过程（见Doersch教程）。VAE的灵感来自Hinton 1995年的唤醒 - 睡眠算法，该算法解决了学习连续潜在变量模型的相同问题。其后代如Beta-VAE（Higgins 2017）可以学习解缠结表示，VQ-VAE（van der Oord 2017）可以进行最先进的图像生成。|
|2013|Christian Szegedy、Wojciech Zaremba、Ilya Sutskever、Joan Bruna、Dumitru Erhan、Ian Goodfellow和Rob Fergus，*Intriguing Properties of Neural Networks*|人工神经网络有缺陷吗？通过简单优化，Szegedy发现很容易构造对抗样本：与自然输入几乎没有区别但能欺骗深度网络错误分类图像的输入。这一观察引发了进一步攻击（如Papernot 2017）、防御（Madry 2018）和评估（Carlini 2017）的发现。|
|2014|Ross Girshick、Jeff Donahue、Trevor Darrell和Jitendra Malik，*Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation*|CNN能否在场景中定位对象？计算机视觉不仅关注分类，还关注在场景中定位和理解对象的排列。通过利用CNN特征的空间排列，Girshick的R-CNN（以及Ren 2015年的更快R-CNN）不仅可以识别对象的类别，还可以通过边界框估计和语义分割识别对象在场景中的位置。|
|2014|Ian J. Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David Warde-Farley、Sherjil Ozair、Aaron Courville、Yoshua Bengio和Jitendra Malik，*Generative Adversarial Nets*|对抗目标能否被学习？生成对抗网络（GAN）通过学习合成能欺骗同时训练以区分真实和生成数据的第二个对抗模型的示例来模仿数据集。这种优雅的方法引发了一波新的理论工作，以及一类新的高度逼真的图像生成方法，如DCGAN（Radford 2016）、Wasserstein GAN（Arjovsky 2017）、BigGAN（Brock 2019）和StyleGAN（Karras 2019）。|
|2014|Jason Yosinksi、Jeff Clune、Yoshua Bengio和Hod Lipson，*How Transferable are Features in Deep Neural Networks?*|网络参数能否在另一个网络中重用？迁移学习使用预训练网络的层来初始化一个训练用于解决不同问题的网络。Yosinksi表明这种微调将优于从头开始训练新网络，从业者很快认识到使用大型预训练模型（PTM）初始化是仅使用少量训练数据获得高性能网络的一种方法。|
|2014|Matthew D. Zeiler和Rob Fergus，*Visualizing and Understanding Convolutional Networks*|人们能否理解深度网络？深度学习的批评之一是其巨大模型对人类来说是不透明的。Zeiler通过回顾和介绍几种深度特征可视化方法来解决这个问题，这些方法描绘网络内的单个信号，以及显著图，它总结了输入中对复杂计算结果影响最大的部分。Zeiler的可解释人工智能（XAI）目标在特征优化方法（Olah 2017）、特征剖析（Bau 2017）以及如Grad - CAM（Selvaraju 2016）和Integrated Gradients（Sundararajan 2017）等显著图方法中得到进一步发展。|
|2014|Ilya Sutskever、Oriol Vinyals和Quoc V. Le，*Sequence to Sequence Learning with Neural Networks*|神经网络能否翻译人类语言？Sutskever将LSTM架构应用于英语到法语的翻译，结合了编码器阶段和自回归解码器阶段。这次神经机器翻译的演示当时并未超越最先进的机器翻译方法，但其具有竞争力的性能确立了神经翻译方法的可行性，这是人工智能的经典重大挑战之一。|
|2015|Dzmitry Bahdanau、KyngHyun Cho和Yoshua Bengio，*Neural Machine Translation by Jointly Learning to Align and Translate*|网络能否学习自己的注意力？虽然CNN比较相邻像素，RNN检查相邻单词，但有时最重要的数据依赖关系并非相邻关系。Bahdanau提出了一种学习注意力模型，可以估计输入的哪些部分与输出的每个部分相关。这一创新极大地提高了神经机器翻译的性能，并且使用可学习注意力的想法被证明对包括图形（Veličković 2018）和图像（Zhang 2019）在内的多种数据有效。|
|2015|Diederik P. Kingma和Jimmy Lei Ba，*Adam: A Method for Stocastic Optimization*|应该使用什么学习率？Adam优化器通过对梯度变化较大区域的参数使用较小步长来自适应选择步长。结合了动量（Polyak 1964）、二阶优化（Becker 1989）、Adagrad（Duchi 2011）、Adadelta（Zeiler 2012）和RMSProp（Tieleman 2012）的思想，Adam优化器在实践中被证明非常有效，能够在几乎无需手动调整的情况下优化巨大模型。|
|2015|Sergei Ioffe和Christian Szegedy，*Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*|如何稳定训练梯度？即使有巧妙的初始化，在非常深的ReLU网络中，最终信号也会变得非常大或非常小。批量归一化通过在每个训练批次内将每个神经元归一化为零均值和单位方差来解决这个问题。这一实际步骤带来了巨大好处，提高了训练速度、网络性能和稳定性，并使非常大的模型能够被训练。|
|2015|Kaiming He、Xiangyu Zhang、Shaoqing Ren和Jian Sun，*Deep Residual Learning for Image Recognition*|如果有大量网络层，反向传播能否成功？通过分析梯度传播，He提出了残差网络（ResNet）架构，其中层计算一个向量添加到信号中，而不是在每层转换信号。他还提出了Kaiming初始化，这是Xavier初始化的一种变体，考虑了非线性因素。与批量归一化一起，这些方法解决了深度问题，使网络能够使用超过100层达到最先进的结果。|
|2015|Oriol Vinyals、Alexander Toshev、Samy Bengio和Dumitru Erhan，*Show and Tell: A Neural Image Caption Generator*|语言和视觉能否相关？这篇论文表明，尽管模态之间存在明显差异，但图像和文本的神经表示可以直接连接。通过简单地将视觉网络（CNN）连接到语言网络（RNN），Vinyals展示了一个系统，该系统可以执行图像字幕生成，在MSCOCO数据集上训练后，为广泛的主题生成准确的字幕。|
|2015|Jascha Sohl - Dickstein、Eric A. Weiss、Niru Maheswaranathan和Surya Ganguli，*Deep Unsupervised Learning using Nonequilibrium Thermodynamics*|网络能否通过逆转扩散物理过程学习？受Kingma的VAE和Hinton的唤醒 - 睡眠方法以及扩散动力学的启发，Jascha Sohl - Dickstein提出了扩散模型，这是一种潜在变量框架，通过学习在许多小步骤中逆转扩散过程，将高斯噪声迭代地转换为有意义的分布。后来，Jonathan Ho（2020）扩展了这个方法，以合成非常高质量的图像，优于GANs，这一演示引发了对使用扩散进行图像合成的兴趣浪潮。有关详细讨论，请参阅Calvin Luo（2022）的教程论文。|
|2016|David Silver、Aja Huang、Chris J. Maddison、Arthur Guez、Laurent Sifre、George van den Driessche、Julian Schrittwieser、Ioannis Antonoglou、Veda Panneershelvam、Marc Lanctot、Sander Dieleman、Dominik Grewe、John Nham、Nal Kalchbrenner、Ilya Sutskever、Timothy Lillicrap、Madeleine Leach、Koray Kavukcuoglu、Thore Graepel和Demis Hassabis，*Mastering the Game of Go with Deep Neural Networks and Tree Search*|游戏是用于展示人工智能能力的原始领域之一。然而，虽然1997年使用传统搜索方法征服了国际象棋，但围棋被认为是一种更为微妙的游戏，直观且难以通过暴力计算解决。在DeepMind的这项工作中，AlphaGo系统将CNN与传统搜索方法相结合，通过强大的学习棋盘评估函数添加所需的直觉，通过自我对弈进行训练。该系统达到了大师级水平，并在五场比赛中击败了排名最高的人类围棋选手李世石。|
|2017|Ashsh Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Łukasz Kaiser、Illia Polosukhin，*Attention is All You Need*|在将Bahdanau的注意力思想应用于实现最先进的机器翻译结果时，Vaswani发现支持递归网络的各种机制是不必要的，可以用注意力代替。由此产生的架构，Transformer网络，被证明是处理序列数据的一种可扩展且通用的方式，导致了诸如BERT、GPT和T5等流行架构的出现。|
|2017|Phillip Isola、Jun - Yan Zhu、Tinghui Zhou和Alexei A. Efros，*Image - to - Image Translation with Conditional Adversarial Networks*|一大类图像处理方法可以被视为从一个图像到另一个图像的转换。在这项工作中，Isola展示了一个单一的Pix2Pix架构可以用于分割、图像重新样式化和草图引导图像生成等问题，通过应用GAN对抗网络训练生成网络创建与目标域匹配的逼真图像。虽然Pix2Pix依赖于成对的前后图像数据集，但它启发了CycleGAN（Zhu，2017），后者能够基于未明确配对的数据学习转换图像。|
|2018|Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei和Ilya Sutskever，*Language Models are Unsupervised Multitask Learners*|网络能否通过阅读学会写作？虽然原始的Transformer架构需要成对的语言翻译文本来进行训练，但Radford丢弃了网络的编码器部分，以获得一个简单的自回归语言模型，该模型可以在预测文本中下一个单词的简单任务上进行训练。由此产生的模型GPT可以扩展到在大量文本上进行训练，并且该模型及其放大的后续版本GPT - 2和GPT - 3表现出涌现行为，例如能够通过用自然语言请求回答特定类型的问题来提示模型解决各种任务。这些大语言模型（LLMs）构成了未来几年人工智能一系列进步的基础。|
|2019|Jacob Devlin、Ming - Wei Chang、Kenton Lee和Kristina Toutanova，*BERT: Pre - training of Deep Bidirectional Transformers for Language Understanding*|是否存在语言的通用编码？虽然传统方法是为解决特定语言问题设计自定义网络，但本文提出了BERT架构，该架构学习以通用方式编码文本。BERT在去噪任务上进行训练，学习填补文本中的缺失单词，并学习区分相邻句子对和不相关句子对。这种无监督训练方案允许BERT扩展并在大量文本上进行训练。BERT使得仅使用少量数据为专门任务创建高质量语言处理模型变得简单，只需从预训练的BERT开始并针对任务进行微调。|
|2020|Ben Mildenhall、Pratul P. Srinivasan、Matthew Tancik、Jonathan T. Barron、Ravi Ramamoorthi和Ren Ng，*NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis*|神经网络能否模拟光传输的物理过程？虽然大多数神经模型受到人类大脑功能的启发，但神经网络可以应用于学习其他领域的功能。在这项工作中，Mildenhall展示了神经辐射场（NeRF），这是一种使用神经网络通过遵循物理规则学习计算3D场景内的全光传输，同时学习匹配在少量照片中观察到的光。通过对体积内每个位置和方向的光量进行建模，NeRF模型能够解决诸如从新视点描绘拍摄场景或显示添加新对象的场景等困难的渲染问题。|
|2020|Andrew W. Senior、Richard Evans、John Jumper、James Kirkpatrick、Laurent Sifre、Tim Green、Chongli Qin、Augustin Žídek、Alexander W. R. Nelson、Alex Bridgland、Hugo Penedones、Stig Petersen、Karen Simonyan、Steve Crossan、Pushmeet Kohli、David T. Jones、David Silver、Koray Kavukcuoglu和Demis Hassabis，*Improved protein structure prediction using potentials from deep learning*|神经网络能否模拟蛋白质结构的物理过程？计算化学的重大挑战之一是从其氨基酸序列预测蛋白质的3D结构，因为该结构对于理解蛋白质的功能至关重要。通过训练卷积神经网络预测公共蛋白质数据库中150,000个已知蛋白质构象的残基距离，DeepMind的AlphaFold极大地改进了蛋白质结构预测的最先进水平；神经方法与其他化学算法相结合以创建完整的3D预测。该团队将他们的方法应用于UniProt数据库中的所有2亿个蛋白质，为生物学家已知的基本上每个蛋白质提供了高置信度预测，改变了分子生物学领域。连同AlphaFold 2（Jumper等人，2021）和AlphaFold 3（Abramson等人，2024），这项工作于2024年获得诺贝尔化学奖。|
|2021|Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel Goh、Sandhini Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark、Gretchen Krueger和Ilya Sutskever，*Learning Transferable Visual Models From Natural Language Supervision*|最好的图像训练数据是否总是需要手动标记？虽然像BERT和GPT这样的大型文本模型在没有手动标签的情况下进行训练，但视觉领域最好的训练数据仍然需要费力地手动标记。这项工作改变了这种情况，展示了一种由从互联网上自动抓取的开放文本图像字幕数据监督的图像表示。CLIP在一个包含4亿个带字幕图像的大规模数据集上应用对比学习，共同学习对齐的图像和文本编码，在零样本测试中接近最先进的分类性能，无需任何微调。CLIP建立了一种新的最先进的图像表示，也是OpenAI的DALL - E文本到图像合成系统的重要组成部分。|
|2022|Robin Rombach、Andreas Blattmann、Dominik Lorenz、Patrick Esser和Björn Ommer，*High - Resolution Image Synthesis With Latent Diffusion Models*|神经网络能否学会绘画？2022年的一系列论文标志着文本到图像合成领域最先进技术的显著进步。通过将扩散模型与使用CLIP表示的文本监督相结合，DALL - E 2（Ramesh等人，2022）展示了一种惊人的能力，即根据文本描述创建图像，这些图像显然是新颖的，但也是现实想法的逼真组合。然后，通过结合来自无分类器指导（Ho和Salimans，2022）的改进文本条件，将扩散堆叠到高效的VAE图像表示上，并在LAION（Schuhmann等人，2022）上训练模型，潜在扩散（开放预训练的Stable Diffusion模型的架构）引发了几个商业和开源文本引导图像合成项目的快速发展，这些项目可实际广泛部署。这项工作引发了关于人工智能在艺术中的使用以及错误信息的新社会问题。|
|2022|Long Ouyang、Jeff Wu、Xu Jiang、Diogo Almeida、Carroll L. Wainwright、Pamela Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray、John Schulman、Jacob Hilton、Fraser Kelton、Luke Miller、Maddie Simens、Amanda Askell、Peter Welinder、Paul Christiano、Jan Leike和Ryan Lowe，*Training language models to follow instructions with human feedback*|神经网络能否像人类一样对话？自艾伦·图灵1950年的模仿游戏以来，人类对话一直是人工智能的基准。在谷歌的FLAN项目（Wei等人，2022）观察到经过微调以遵循自然语言的大语言模型（LLM）具有惊人的泛化能力后，OpenAI的人工智能安全团队开发了一种可扩展的微调方法，通过应用强化学习从人类反馈（RLHF，Christiano等人，2017）来训练LLM在对话中符合人类偏好。由此产生的产品ChatGPT作为发布，出色地通过了图灵测试，并改变了世界对人工智能的看法。Wei等人（2022b）观察到思维链提示进一步加强了推理，微软研究人员Bubek等人（2023）认为该系统显示出“通用人工智能的火花”，即在广泛任务上具有人类水平的推理能力。ChatGPT激发了大量商业竞争对手，并且提出了RLHF的变体，包括Constitutional RL（Bai，2022），它使用LLM检查与人类指令的一致性，以及DPO（Rafailov等人，2023），它提出了一种更简单的微调LLM的训练目标。|




# 深度学习论文成千上万，从哪里开始呢？

papers.baulab.info/
东北大学David Bau教授整理的一份深度学习论文列表，大概50多篇，每一篇都对于深度学习领域的发展起到了关键性的推动作用。



This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. 

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------
